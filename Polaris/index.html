<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Polaris: Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models">
    <meta name="keywords" content="Robotic perception, Large language models, Robotic manipulation">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models</title>
    <link rel="icon" type="image/png" href="./resources/images/icon_polaris.png">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <a class="navbar-item" href="https://star-uu-wang.github.io/">
          <span class="icon">
              <i class="fas fa-home"></i>
          </span>
          </a>

          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link">
              More Research
            </a>
            <div class="navbar-dropdown">
              <a class="navbar-item" href="https://star-uu-wang.github.io/">
                SAR-Net: Shape Alignment and Recovery Network for Category-level 6D Object Pose and Size Estimation
              </a>
            </div>
          </div>
        </div>

      </div>
    </nav> -->
    <style>
		.render_wrapper {
			position: relative;
            height: 300px;
         }
        .render_wrapper_small {
			position: relative;
            height: 200px;
         }
		.render_div {
			position: absolute;
			top: 0;
			left: 0;
		}

        #interpolation-image-wrapper-car{
            text-align: center;
        }
        #interpolation-image-wrapper-chair{
            text-align: center;
        }
        .nested-columns {
            margin-bottom: 0 !important;
        }
    </style>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body" style="padding-top: 10px; padding-bottom: 10px;">
            <div class="container is-max-widescreen">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <div class="column is-full_width">
                            <h1 class="title is-1 publication-title"><p><img src="./resources/images/icon_polaris.png" id="pourit" width="5%"><span style="color: rgb(194, 14, 238);">Polaris</span></p> <p>O<span style="color: rgb(194, 14, 238);">p</span>en-ended Interactive R<span style="color: rgb(194, 14, 238);">o</span>botic Manipu<span style="color: rgb(194, 14, 238);">la</span>tion</p> <p>via Syn2<span style="color: rgb(194, 14, 238);">R</span>eal V<span style="color: rgb(194, 14, 238);">is</span>ual Grounding and</p> <p>Large Language Models</p></h1>
                        </div>

                        <div class="column is-full_width">
                          <h2 class="title is-4">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024</h2>
                        </div>

                        <div class="is-size-3 publication-authors">
                            <span class="author-block">
                                <a href="https://star-uu-wang.github.io/">Tianyu Wang</a>,</span>
                            <span class="author-block">
                                <a href="http://hetolin.github.io/">Haitao Lin</a>,</span>
                            <span class="author-block">
                                <a href=" ">Junqiu Yu</a>,</span>   
                            <span class="author-block">
                                <a href="http://yanweifu.github.io">Yanwei Fu</a></span>
                        </div>

                        <div class="is-size-3 publication-authors">
                            <span class="author-block">Fudan University</span>
                        </div>
                        <div style="text-align:center">
                            <img src="./resources/images/FDU.png" width="15%" />
                        </div>
                        <div class="column has-text-centered">
                            <div class="publication-links">

                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="./resources/paper/2408.07975v1.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a class="external-link button is-normal is-rounded is-dark" href="https://arxiv.org/abs/2408.07975">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- <span class="link-block">
                                    <a href="./resources/images/arXiv_pre.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Supp</span>
                                    </a>
                                </span> -->

                                <!-- Video Link. -->
                                <span class="link-block">
                                    <a href="https://www.bilibili.com/video/BV1mbesefE71/?spm_id_from=333.999.0.0&vd_source=dc0dce7cc51928b7de4b2975fb6177e4"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <svg class="svg-inline--fa fa-youtube fa-w-18" aria-hidden="true"
                                                focusable="false" data-prefix="fab" data-icon="youtube" role="img"
                                                xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"
                                                data-fa-i2svg="">
                                                <path fill="currentColor"
                                                    d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z">
                                                </path>
                                            </svg><!-- <i class="fab fa-youtube"></i> Font Awesome fontawesome.com -->
                                        </span>
                                        <span>Video</span>
                                    </a>
                                </span>

                                <!-- Github Link. -->
                                <span class="link-block">
                                    <a href=" " class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code(Coming Soon)</span>
                                    </a>
                                </span>

                                <!-- Dataset Link. -->
                                <span class="link-block">
                                  <a href=" "
                                     class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="far fa-images"></i>
                                    </span>
                                    <span>Data(Coming Soon)</span>
                                    </a>


                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <div align="center"><img src="./resources/images/teaser_polaris.png" width="100%" /></div>
                <h2 class=" subtitle has-text-centered" style="padding-top: 10px">
                    <span style="color: rgb(194, 14, 238);">Polaris</span>: A tabletop-level object robotic manipulation framework centered on syn2real visual grounding driven by open-ended interaction with GPT-4. Users engage in continuous, open-ended interaction with LLM, which maintains an ongoing comprehension of the scenes. 3D synthetic data is integrated into the training of grounded vision modules to facilitate the execution of real-world robotic tasks.
                </h2>
            </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            This paper investigates the task of the <b>open-ended interactive robotic manipulation on table-top scenarios</b>. 
                        </p>

                        <p>
                            While recent Large Language Models (LLMs) enhance robots' comprehension of user instructions, their lack of visual grounding constrains their ability to physically interact with the environment. This is because the robot needs to locate the target object for manipulation within the physical workspace. To this end, we introduce an interactive robotic manipulation framework called <span style="color: rgb(194, 14, 238);">Polaris</span>, which integrates perception and interaction by utilizing GPT-4 alongside grounded vision models.
                        </p>

                        <p>
                            For precise manipulation, it is essential that such grounded vision models produce detailed object pose for the target object, rather than merely identifying pixels belonging to them in the image. Consequently, we propose a novel Synthetic-to-Real (Syn2Real) pose estimation pipeline. This pipeline utilizes rendered synthetic data for training and is then transferred to real-world manipulation tasks. The real-world performance demonstrates the efficacy of our proposed pipeline and underscores its potential for extension to more general categories. Moreover, real-robot experiments have showcased the impressive performance of our framework in grasping and executing multiple manipulation tasks. This indicates its potential to generalize to scenarios beyond the tabletop. 
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- Paper video. -->
        <div align="center">
            <img src="./resources/images/icon_polaris.png" width="15%" />
        </div>
        <div class="columns is-centered has-text-centered">
            <div class="column is-three-fifths">
<!--                <h2 class="title is-3">Video (BiliBili)</h2>-->
                <div class="publication-video">
<!--                <div style="position: relative; padding: 30% 45%;">-->
                    <iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="https://player.bilibili.com/player.html?isOutside=true&aid=112963830681149&bvid=BV1mbesefE71&cid=500001649940403&p=1&high_quality=1&danmaku=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
                </div>
                <br>

                </br>
                <div class="publication-video">
<!--                <div style="position: relative; padding: 30% 45%;">-->
                   <iframe width="1254" height="705" src="https://www.youtube.com/embed/UXtXFkCjIbo" title="Polaris (IROS2024)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>
            </div>

        </div>
        <!--/ Paper video. -->
    </section>


    <section class="section">
        <div class="container is-max-desktop">

            <!-- Overview. -->
            <div class="columns is-centered" style="margin-top: 15px">
                <div class="column is-full-width">
                    <h2 class="title is-3">Method Overview</h2>
                    <img src="./resources/images/overview_polaris.png"/>
                    <div class="content has-text-justified" style="padding-top: 15px">
                        <p>
                            Overview of <span style="color: rgb(194, 14, 238);">Polaris</span>.
                            (a) 3D synthetic data rendering. During rendering, we automatically generate various synthetic data by loading 3D model assets into a simulation engine and deploying dynamic virtual camera. We use the Fibonacci Sphere Sampling to select rendering viewpoints, to generate corresponding RGB, depth, pose, and observable point clouds. 
                            (b) The vision-centric robotic task pipeline. Given the image of the scene, which GPT-4, prompted as a scene perception and interaction LLM, interprets to understand instructions and describe objects and tasks. Our parser interprets these descriptions. We freeze the pre-trained detector and segmentation model within the grounded vision models and use a synthetic dataset to train the category-level pose estimation model. After retrieving object attributes, the model predicts poses based on the scene, allowing a 6D pose robot manipulation planner to execute real-world tasks.
                        </p>
                    </div>

                </div>
            </div>

        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">

            <!-- Overview. -->
            <div class="columns is-centered" style="margin-top: 15px">
                <div class="column is-full-width">
                    <h2 class="title is-3">Example Qualitative Results of our Syn2Real Pose Estimation</h2>
                    <div align="center">
                        <img src="./resources/images/pose_single.png" width="80%"/>
                        <div class="content has-text-justified" style="padding-top: 15px">
                            <p>
                                Test results of single-object scene. We present a subset of the visualization results of the pose and size estimation using the trained MVPoseNet6D model. The outcomes are represented with a tightly oriented 3D bounding box and colored XYZ-axis.
                            </p>
                        </div>

                        <img src="./resources/images/pose_view.png" width="80%"/>
                        <div class="content has-text-justified" style="padding-top: 15px">
                            <p>
                                The scene with same object under multiple views. We show the pose of a bottle under different views.
                            </p>
                        </div>

                        <img src="./resources/images/pose_clutter.png" width="80%"/>
                        <div class="content has-text-justified" style="padding-top: 15px">
                            <p>
                                The scene with multiple objects under the same view. We show the pose estimation of different objects in several cluttered scenes.
                            </p>
                        </div>  
                    </div>


                </div>
            </div>

        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">

            <!-- Overview. -->
            <div class="columns is-centered" style="margin-top: 15px">
                <div class="column is-full-width">
                    <h2 class="title is-3">Examples of Open-ended Interactive Real-robot Experiments</h2>
                    <div align="center">
                        <img src="./resources/images/polaris_experiments.png" width="100%"/>
                        <div class="content has-text-justified" style="padding-top: 15px">
                            <p>
                                Manipulation tasks for three different base scenes are presented, including excerpts from the interaction process between the user and the LLM, the pose estimation results of the manipulated objects in different scenes, and  the keyframes of the robot manipulation. <b>Scene A:</b> Stack bottles on the table. <b>Scene B:</b> Tidy the items of workbench. <b>Scene C:</b> A compositional task considering the affordance of objects after a sudden collision.
                            </p>
                        </div>  
                    </div>


                </div>
            </div>

        </div>
    </section>

<!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{lin2023pourit,
  title={PourIt!: Weakly-supervised Liquid Perception from a Single Image for Visual Closed-Loop Robotic Pouring},
  author={Lin, Haitao and Fu, Yanwei and Xue, Xiangyang},
  journal={International Conference on Computer Vision (ICCV)},
  year={2023}, -->
<!--  url={https://arxiv.org/abs/2207.13691},-->
<!-- }</code></pre>
    </div>
</section> -->


    <section class="section">
        <div class="container is-max-desktop content">
            <h2 class="title">Acknowledgements</h2>
            We thank CFFF platform of Fudan University, and <a href="https://sapien.ucsd.edu/">SAPIEN</a> for providing lightweight rendering engine. This work was supported in part by Shanghai Platform for Neuromorphic and AI Chip under Grant 17DZ2260900 (NeuHelium).
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <!-- <div class="content has-text-centered">
                <a class="icon-link" href="#">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="#" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div> -->
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p style="text-align:center">
                            Source code mainly borrowed from <a href="https://niessnerlab.org/members/yawar_siddiqui/profile.html">Yawar Siddiqui</a>'s <a
                                href="https://nihalsid.github.io/texturify/">Texturify website</a> and <a href="https://hetolin.github.io/">Haitao Lin</a>'s <a href="https://hetolin.github.io/PourIt/">PourIt website</a>
                        </p>
                        <p style="text-align:center">
                            Please contact Tianyu Wang: <span style="color: gray;">tywang22@m.fudan.edu.cn</span> or Professor Yanwei Fu: <span style="color: gray;">yanweifu@fudan.edu.cn</span> for feedback and questions.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Import maps polyfill -->
    <!-- Remove this when import maps will be widely supported -->
    <script async src="https://unpkg.com/es-module-shims@1.3.6/dist/es-module-shims.js"></script>

    <script type="importmap">
        {
            "imports": {
                "three": "./js/three.module.js"
            }
        }
    </script>

    <script type="module">

        import * as THREE from 'three';

        import { PLYLoader } from './js/PLYLoader.js';
        import { OBJLoader } from './js/OBJLoader.js';
        import { MTLLoader } from './js/MTLLoader.js';
        import { OrbitControls } from './js/OrbitControls.js'
        let div_to_scene = {
            "compass": {},
            "ambulance": {},
            "backpack": {},
            "dumpster": {},
            "baroque": {},
            "wooden": {},
            "cyberpunk": {},
            "lego": {},
            "airplane": {},
            "crocodile": {},
            "hippo": {},
            "sheep": {}
        };
        let div_camera = {
            "compass": null,
            "ambulance": null,
            "backpack": null,
            "dumpster": null,
            "baroque": null,
            "wooden": null,
            "cyberpunk": null,
            "lego": null,
            "airplane": null,
            "crocodile": null,
            "hippo": null,
            "sheep": null,
        };

        let mouse_button_down = false;
        let list_of_orbit_controls = []
        let style_camera = null;
        let render_colors = true;
        let style_id = "0"

        function setup_camera(div_name){
            let container = document.getElementById(div_name);
            let width = container.parentElement.clientWidth;
            let height = container.parentElement.clientHeight;
            console.log(width, height)
            let camera = new THREE.PerspectiveCamera( 35, width / height, 0.1, 50 );
            let camera_init_position = new THREE.Vector3( -1, 0.5, 1 );
            if (div_name.includes("compass")){
                camera_init_position = new THREE.Vector3( 1, 1.5, 1 );
            }
            else if (div_name.includes("ambulance")) {
                camera_init_position = new THREE.Vector3( -1.1, 0.5, 1.1 );
            }
            // else if (div_name.includes("backpack")) {
            //     camera_init_position = new THREE.Vector3( -1.5, 0.5, 1.5 );
            // }
            camera.position.set(camera_init_position.x, camera_init_position.y, camera_init_position.z);
            return camera;
        }

        function setup_render_divs(div_name, obj_name, obj_path, mtl_path, texture_path){
            if (div_camera[obj_name] == null) {
                div_camera[obj_name] = setup_camera(div_name)
            }

            let orbit_control = create_render_div(div_camera[obj_name], div_name, obj_name, obj_path, mtl_path, texture_path)
            list_of_orbit_controls.push(orbit_control)
        }

        function create_render_div(camera, div_id, obj_name, obj_path, mtl_path, texture_path) {
            let container;
            let renderer, controls;

            init();
            animate();

            function init() {

                container = document.getElementById(div_id);
                let width = container.parentElement.clientWidth;
                let height = container.parentElement.clientHeight;


                div_to_scene[obj_name][div_id] = new THREE.Scene();
                div_to_scene[obj_name][div_id].background = new THREE.Color( 0xffffff );

                // ObJ file

                const objLoader  = new OBJLoader();
                const textureLoader = new THREE.TextureLoader();
                const mtlLoader = new MTLLoader();

                mtlLoader.load(mtl_path, function ( materials ) {
                    materials.preload();
                    objLoader
                        .setMaterials( materials )
                        .load( obj_path, function ( object ) {
                            textureLoader
                                .load( texture_path, function ( texture ) {
                                    object.traverse (function ( child ) {
                                        if ( child instanceof THREE.Mesh ) {
                                            child.material.map = texture;
                                        }
                                    });

                                    div_to_scene[obj_name][div_id].add( object );
                                });
                        });
                });

                div_to_scene[obj_name][div_id].add( new THREE.HemisphereLight( 0x333333, 0x222222 ) );
                addShadowedLight(div_to_scene[obj_name][div_id], 1, 1, 1, 0xffffff, 1 );
                addShadowedLight(div_to_scene[obj_name][div_id],  -1, -1, - 1, 0xffffff, 0.5 );

                // renderer

                renderer = new THREE.WebGLRenderer( { antialias: true } );
                renderer.setPixelRatio( window.devicePixelRatio );
                renderer.setSize( width, height);
                renderer.outputEncoding = THREE.sRGBEncoding;

                renderer.shadowMap.enabled = true;

                container.appendChild( renderer.domElement );

                controls = new OrbitControls(camera, renderer.domElement)
                controls.enableDamping = false

                // resize

                window.addEventListener( 'resize', onWindowResize );

        }
            function onWindowResize() {
                let width = container.clientWidth;
                let height = container.clientHeight;
                camera.aspect = width / height;
                camera.updateProjectionMatrix();
                renderer.setSize( width, height );
            }
            function animate() {
                requestAnimationFrame( animate );
                render();
            }

            function render() {
                renderer.render( div_to_scene[obj_name][div_id], camera );
                controls.update();
            }

            return controls;
        }

        function addShadowedLight(scene, x, y, z, color, intensity ) {

            const directionalLight = new THREE.DirectionalLight( color, intensity );
            directionalLight.position.set( x, y, z );
            scene.add( directionalLight );

            directionalLight.castShadow = true;

            const d = 1;
            directionalLight.shadow.camera.left = - d;
            directionalLight.shadow.camera.right = d;
            directionalLight.shadow.camera.top = d;
            directionalLight.shadow.camera.bottom = - d;

            directionalLight.shadow.camera.near = 1;
            directionalLight.shadow.camera.far = 4;

            directionalLight.shadow.mapSize.width = 1024;
            directionalLight.shadow.mapSize.height = 1024;

            directionalLight.shadow.bias = - 0.001;

        }

        document.addEventListener('keydown', logKey);

        function logKey(evt) {
            if (evt.keyCode === 82 && !mouse_button_down) {
                reset_orbit_controls()
            }
        }

        function reset_orbit_controls() {
            list_of_orbit_controls.forEach(oc => {
                oc.reset()
            })
        }

        document.body.onmousedown = function(evt) {
            if (evt.button === 0)
                mouse_button_down = true
        }
        document.body.onmouseup = function(evt) {
            if (evt.button === 0)
                mouse_button_down = false
        }

        window.onload = function() {
            let slider = document.getElementsByClassName("slider")[0]
            slider.removeAttribute("tabIndex")
            // slider.addEventListener("mouseout", reset_orbit_controls);

            // const qualitative = ["compass", "ambulance", "backpack", "dumpster"];
            const qualitative = ["compass", "ambulance"];

            qualitative.forEach(element => {
                setup_render_divs(element+"_geo", element,
                './samples/qualitative/'+element+'/geo/mesh_normalized.obj',
                './samples/qualitative/'+element+'/geo/mesh_normalized.mtl',
                './samples/qualitative/'+element+'/geo/mesh_normalized.png'
                );
                setup_render_divs(element+"_clipmesh", element,
                    './samples/qualitative/'+element+'/clipmesh/mesh_normalized.obj',
                    './samples/qualitative/'+element+'/clipmesh/mesh_normalized.mtl',
                    './samples/qualitative/'+element+'/clipmesh/mesh_normalized.png'
                );
                setup_render_divs(element+"_latentpaint", element,
                    './samples/qualitative/'+element+'/latentpaint/mesh_normalized.obj',
                    './samples/qualitative/'+element+'/latentpaint/mesh_normalized.mtl',
                    './samples/qualitative/'+element+'/latentpaint/mesh_normalized.png'
                );
                setup_render_divs(element+"_ours", element,
                    './samples/qualitative/'+element+'/ours/mesh_normalized.obj',
                    './samples/qualitative/'+element+'/ours/mesh_normalized.mtl',
                    './samples/qualitative/'+element+'/ours/mesh_normalized.png'
                );
            });


            const styles = ["baroque", "wooden", "cyberpunk", "lego"];
            styles.forEach(element => {
                setup_render_divs(element+"_porsche", element,
                './samples/different_style/'+element+'_porsche/mesh_normalized.obj',
                './samples/different_style/'+element+'_porsche/mesh_normalized.mtl',
                './samples/different_style/'+element+'_porsche/mesh_normalized.png'
                );
            });

            const others = ["airplane", "crocodile", "hippo", "sheep"];
            others.forEach(element => {
                setup_render_divs(element+"_porsche", element,
                './samples/wrong_prompt/'+element+'/mesh_normalized.obj',
                './samples/wrong_prompt/'+element+'/mesh_normalized.mtl',
                './samples/wrong_prompt/'+element+'/mesh_normalized.png'
                );
            });

        };

    </script>
</body>

</html>