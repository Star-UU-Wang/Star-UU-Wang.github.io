<!DOCTYPE HTML>
<html>
    <head>
        <title>Tianyu Wang</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=1000">
        <link rel="stylesheet" href="https://use.typekit.net/quv7bsd.css"> <!-- fonts -->
        <link rel="stylesheet" href="style.css" />
        <link rel="icon" type="image/png" href="images/brain_icon.png">

        <script>
             (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
             (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
             m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
             })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
             ga('create', 'UA-89797207-1', 'auto');
             ga('send', 'pageview');
       </script>
    </head>
    <body id="body">
        <div id="main"> 
            <header id="header">
                <a href="index.html">HOME</a>&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;
                <a href="https://www.youtube.com/channel/UCtFUWbGM1KWANiiwyTMzTXg">CHANNEL</a>
            </header>
            <div id="profile">
                <div id="profile-pic">
                    <img src="profile/identity.jpg">
                    <p>
                        <a href="https://scholar.google.com/citations?user=YnSQm94AAAAJ&hl=en">Google Scholar</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://github.com/Star-UU-Wang">Github</a>
                    </p>
                </div>
                <div id="profile-intro">
                    <div id="profile-name">Tianyu Wang | ÁéãÂ§©ÂÆá</div>
                    <p style="text-align:justify; text-justify:inter-ideograph;"> 
                        I am currently a second-year Ph.D. student majoring in Biomedical Engineering at <a href="https://istbi.fudan.edu.cn"> Institute of Science and Technology for Brain-Inspired Intelligence (ISTBI), Fudan University</a>. I am honored to be advised by Prof. <a href="https://yanweifu.github.io"> Yanwei Fu</a> and Prof. <a href="https://istbi.fudan.edu.cn/lnen/info/1157/1632.htm">Shouyan Wang</a>. Previously, I received the Bachelor's degree (June 2022) in Software Engineering from Nanjing University of Information Science and Technology (NUIST), under the supervision of Prof. <a href="https://sites.google.com/view/xiaolongxu">Xiaolong Xu</a>.<br><br>
                        
                        My research interests span <span style="color: #003373">Embodied Intelligence</span> and <span style="color: #003373">Computational Neuroscience</span>, e.g., robotic manipulation based on 3D computer vision, unified modeling of cognition and behavior. At present, I devote to enhancing the robotic perception capabilities through 3D reconstruction.<br><br>

                        In recent years, I would like to thank Dr. <a href="https://hetolin.github.io/">Haitao Lin</a> and Dr. <a href="https://naiq.github.io/">Xuelin Qian</a> for their support during my research, as well as my co-workers.<br>
                        <!-- I am also grateful to Miss Huamin Zhou for the enduring companionship and support. -->
                        <span style="color: #003373">Email</span>: tywang22@m.fudan.edu.cn
                    </p>
                </div>
                <div style="clear: both;"></div>
            </div>

            <div class="divider"></div>
            <div class="section news">
                <h1>News</h1>
                <p>
                    <ul>
                        <li> [2025/02] üéâ Two papers get accepted to <a href="https://cvpr.thecvf.com/">CVPR 2025</a>. </li>
                        <li> [2025/02] We won the third prize in the first Global Lerobot Embodied Intelligence Hackathon. </li>
                        <li> [2025/01] üéâ One paper gets accepted to <a href="https://2025.ieee-icra.org/">ICRA 2025</a>. </li>
                        <li> [2024/10] Excited to announce ‚ö°Ô∏è<a href="https://fastumi.com/" style="color: #fdc450;">Fast-UMI</a>, a scalable and hardware-independent universal manipulation interface (Open Source Project). </li>
                        <li> [2024/09] üéâ One paper gets accepted to <a href="https://www.corl.org/">CoRL 2024</a>. </li>
                        <li> [2024/08] I have joined the <a href="https://www.shlab.org.cn/">Shanghai AI Lab</a> as an Embodied AI Intern, aiming to conduct research on learning for manipulation, under the guidance of Dr. <a href="https://yding25.com/">Yan Ding</a>. Looking forward to this new journey! [Project completed in 2024/11] </li>
                        <li> [2024/06] üéâ One paper gets accepted to <a href="https://iros2024-abudhabi.org/">IROS 2024</a>. </li>
                    	<!-- <li> [2023/05] I joined <a href="https://www.bosch.com/">Bosch</a> as a strategic intern to carry out research on robotic perception, where I will spend the summer. A new journey begins!</li>
                        <li> [2023/11] We have accomplished an effort to apply pure synthetic data to the real world! </li> -->
                        <li> [2023/12] I have joined the Shanghai Computer Society. </li>
                        <li> [2023/06] I started my research on interactive general robotic grasping in collaboration with Dr. <a href="https://swuhk.github.io/">Shuang Wu</a> from Huawei Noah's Ark Lab, and I am grateful for some of his guidance. [Project completed in 2024/06] </li>
                    </ul>
                </p>
                <div style="clear: both;"></div>
            </div>
            <!-- <div class="divider"></div> -->

            <div class="divider"></div>
            <div class="section research">
                <h1>Research</h1>
                <h3> * denotes equal contribution</h3>

                <!-- <h3><div id="repBtn"><a href="javascript:showRep()">Representative</a></div>&nbsp;&nbsp;&nbsp;&bull;&nbsp;&nbsp;&nbsp; -->
                <!-- <div id="showAllBtn"><a href="javascript:hideRep()">See All Publications</a></div></h3> -->
                <div class="research-proj">
                    <a href="https://sressers.github.io/RAG-6DPose/" class="research-thumb">
                    <img src="images/paper/RAG6DPose.png" alt="" />
                    </a>


                    <a href="https://sressers.github.io/RAG-6DPose/" class="research-proj-title"> RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base </a>
                    <p> Kuanning Wang, Yuqian Fu, <strong>Tianyu Wang</strong>, Yanwei Fu, Longfei Liang, Yu-Gang Jiang, Xiangyang Xue <br>
                       IROS 2025 <br>
                       <a href="https://sressers.github.io/RAG-6DPose/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2506.18856">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href=" ">Code </a> 
                    </p>
                    <!-- <p> <br> </p> -->
                </div>

                <div class="research-proj">
                    <a href="https://star-uu-wang.github.io/HOP/" class="research-thumb">
                    <img src="images/paper/cvpr2025_hop.gif" alt="" />
                    </a>


                    <a href="https://star-uu-wang.github.io/HOP/" class="research-proj-title"> HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation </a>
                    <p> Hongye Cheng<sup>*</sup>, <strong>Tianyu Wang</strong><sup>*</sup>, Guangsi Shi, Zexing Zhao, Yanwei Fu <br>
                       CVPR 2025 <br>
                       <a href="https://star-uu-wang.github.io/HOP/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2503.01175">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/Chenghyyy/HOP-Heterogeneous-Topology-based-Multimodal-Entanglement-for-Co-Speech-Gesture-Generation">Code </a> 
                    </p>
                    <!-- <p> <br> </p> -->
                </div>

                <div class="research-proj">
                    <a href="https://shanehuanghz.github.io/CAPNet/" class="research-thumb">
                    <img src="images/paper/cvpr2025_capnet.png" alt="" />
                    </a>


                    <a href="https://shanehuanghz.github.io/CAPNet/" class="research-proj-title"> CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image </a>
                    <p> Jingshun Huang<sup>*</sup>, Haitao Lin<sup>*</sup>, <strong>Tianyu Wang</strong>, Yanwei Fu, Xiangyang Xue, Yi Zhu <br>
                       CVPR 2025 <highlight>(Highlight)</highlight> <br>
                       <a href="https://shanehuanghz.github.io/CAPNet/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2504.11230">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://github.com/ShaneHuangHZ/CAPNet">Code </a> 
                    </p>
                    <!-- <p> <br> </p> -->
                </div>

                <div class="research-proj">
                    <a href="https://shanehuanghz.github.io/YOEO/" class="research-thumb">
                    <img src="images/paper/icra2025_yoeo.gif" alt="" />
                    </a>


                    <a href="https://shanehuanghz.github.io/YOEO/" class="research-proj-title"> You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping </a>
                    <p> Jingshun Huang, Haitao Lin, <strong>Tianyu Wang</strong>, Yanwei Fu, Yu-Gang Jiang, Xiangyang Xue <br>
                       ICRA 2025 <br>
                       <a href="https://shanehuanghz.github.io/YOEO/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href=" ">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href=" ">Code </a> 
                    </p>
                    <!-- <p> <br> </p> -->
                </div>
                
                <div class="research-proj">
                    <a href="https://jarvishou829.github.io/TaMMa/" class="research-thumb">
                    <img src="images/paper/corl2024_tamma.gif" alt="" />
                    </a>


                    <a href="https://jarvishou829.github.io/TaMMa/" class="research-proj-title"> TaMMa: Target-driven Multi-subscene Mobile Manipulation </a>
                    <p> Jiawei Hou<sup>*</sup>, <strong>Tianyu Wang</strong><sup>*</sup>, Tongying Pan<sup>*</sup>, Shouyan Wang, Xiangyang Xue, Yanwei Fu <br>
                       CoRL 2024 <br>
                       <a href="https://jarvishou829.github.io/TaMMa/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href=" ">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href=" ">Code </a> 
                    </p>
                    <!-- <p> <br> </p> -->
                </div>
                
                <div class="research-proj">
                    <a href="https://fastumi.com/" class="research-thumb">
                    <img src="images/paper/fastumi.png" alt="" />
                    </a>


                    <a href="https://fastumi.com/" class="research-proj-title"> Fast-UMI: A Scalable and Hardware-Independent Universal Manipulation Interface </a>
                    <p> Ziniu Wu<sup>*</sup>, <strong>Tianyu Wang</strong><sup>*</sup>, Zhaxizhuoma<sup>*</sup>, Chuyue Guan, Zhongjie Jia, Dong Wang, Nieqing Cao, Yan Ding, Bin Zhao, Xuelong Li <br>
                        Pre-print 2024 <br>
                       <a href="https://fastumi.com/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://www.arxiv.org/abs/2409.19499">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://fastumi.com/appendix">Supplementary (released)</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href=" ">Code </a> 
                    </p>
                    <!-- <p> <br> </p> -->
                </div>

                <div class="research-proj">
                    <a href="https://star-uu-wang.github.io/Polaris/" class="research-thumb">
                    <img src="images/paper/iros2024_polaris.gif" alt="" />
                    </a>


                    <a href="https://star-uu-wang.github.io/Polaris/" class="research-proj-title"> Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models </a>
                    <p> <strong>Tianyu Wang</strong>, Haitao Lin, Junqiu Yu, Yanwei Fu <br>
                       IROS 2024 <highlight>(Oral Pitch)</highlight> <br>
                       <a href="https://star-uu-wang.github.io/Polaris/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2408.07975">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href=" ">Code </a> 
                    </p>
                    <!-- <p> <br> </p> -->
                </div>

                <div class="research-proj">
                    <a href="https://star-uu-wang.github.io/WALL-E/" class="research-thumb">
                    <img src="images/paper/walle.png" alt="" />
                    </a>


                    <a href="https://star-uu-wang.github.io/WALL-E/" class="research-proj-title"> WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Models </a>
                    <p> <strong>Tianyu Wang</strong>, Yifan Li, Haitao Lin, Jingshun Huang, Xiangyang Xue, Yanwei Fu <br>
                       Pre-print 2023 <br>
                       <a href="https://star-uu-wang.github.io/WALL-E/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href="https://arxiv.org/abs/2308.15962">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                       <a href=" ">Code </a> 
                    </p>
                    <!-- <p> <br> </p> -->
                </div>

                <div class="research-proj">
                    <a href=" " class="research-thumb">
                    <img src="images/paper/UAV deployment.png" alt="" />
                    </a>


                    <a href="https://link.springer.com/article/10.1007/s11276-021-02777-x" class="research-proj-title"> UAV deployment with grid modeling and adaptive multiple pruning search in complex forest scenarios </a>
                    <p> <strong>Tianyu Wang</strong>, Wei Gu <br>
                       Wireless Networks 2021 <br>
                       <a href="https://link.springer.com/article/10.1007/s11276-021-02777-x">Paper</a>
                    </p>
                </div>

                <!-- <div onclick="togglePubs()" id="morePubsBtn" class="showBtn"><a>Show more...</a></div>
                <div onclick="togglePubs()" id="lessPubsBtn" class="showBtn"><a>Show less...</a></div>
                <div style="clear: both;"></div> -->
            </div>

            <div class="divider"></div>
            <div class="section projects">
                <h1>Projects</h1>

                <!-- <h3><div id="repBtn"><a href="javascript:showRep()">Representative</a></div>&nbsp;&nbsp;&nbsp;&bull;&nbsp;&nbsp;&nbsp; -->
                <!-- <div id="showAllBtn"><a href="javascript:hideRep()">See All Publications</a></div></h3> -->
                <div class="research-proj">
                    <a class="research-thumb">
                    <video playsinline="" muted="" autoplay="" loop="" width="180px">
                    <source src="videos/TeaBot_demo.mp4" type="video/mp4">
                    </video>
                    </a>


                    <a href=" " class="research-proj-title"> TeaBot </a>
                    <p> TeaBot backed by <a style="color: rgb(255, 209, 28);" href="https://x.com/lerobothf">LeRobot</a>: The core action sequence of making tea, including pouring tea leaves, adding water, and stirring, is attempted using a general robotic arm. <br>
                        <span style="color: gray;">Third prize</span> in the first Global Lerobot Embodied Intelligence Hackathon, hosted by <a style="color: rgb(141, 194, 31);" href="https://x.com/seeedstudio">Seeed Studio</a> - 2025 <br>
                    </p>
                    <!-- <p> <br> </p> -->
                </div>

            </div>

            <div class="divider"></div>
            <div class="section experience">
                <h1>Experience</h1>

                <div class="section experience-item">
                <!-- badge -->
                    <img src="images/badge/FDU.png" alt="FDU" class="institute-logo">
            
                <!-- content -->
                    <div class="experience-content">
                        <h3><a href="https://www.fudan.edu.cn/en/" class="institute-title">Fudan University (FDU)</a></h3>
                        <p class="time">2022.09 - Present&nbsp;&nbsp;&bull;&nbsp;&nbsp;<span>Postgraduate student</span></p>
                        <p>Research Advisor: Prof. <a href="https://yanweifu.github.io">Yanwei Fu</a> & Prof. <a href="https://istbi.fudan.edu.cn/lnen/info/1157/1632.htm">Shouyan Wang</a></p>
                    </div>
                </div>

                <div class="section experience-item">
                    <!-- badge -->
                        <img src="images/badge/ShanghaiAILab.png" alt="Shanghai AI Laboratory" class="institute-logo">
                
                    <!-- content -->
                        <div class="experience-content">
                            <h3><a href="https://www.shlab.org.cn/" class="institute-title">Shanghai AI Laboratory</a></h3>
                            <p class="time">2024.08 - 2024.11&nbsp;&nbsp;&bull;&nbsp;&nbsp;<span>Research Intern</span></p>
                            <p>Mentor: Dr. <a href="https://yding25.com/">Yan Ding</a></p>
                        </div>
                </div>

                <div class="section experience-item">
                    <!-- badge -->
                        <img src="images/badge/YOFO.png" alt="YOFO" class="institute-logo">
                
                    <!-- content -->
                        <div class="experience-content">
                            <h3><a href="http://www.yoforobot.com/" class="institute-title">YOFO Robot</a></h3>
                            <p class="time">2022.10 - 2023.03&nbsp;&nbsp;&bull;&nbsp;&nbsp;<span>R&D Intern</span></p>
                            <p>Mentor: Dr. Zhenfeng Sun (CEO of <a href="http://www.yoforobot.com/">YOFO Robot</a>)</p>
                        </div>
                </div>

                <div class="section experience-item">
                    <!-- badge -->
                        <img src="images/badge/Cambridge.png" alt="Cambridge" class="institute-logo">
                
                    <!-- content -->
                        <div class="experience-content">
                            <h3><a href="https://www.cam.ac.uk/" class="institute-title">University of Cambridge</a></h3>
                            <p class="time">2019.01 - 2019.03&nbsp;&nbsp;&bull;&nbsp;&nbsp;<span>Visiting student at Hughes Hall</span></p>
                            <p>Cambridge Future Scientists Winter Programme</p>
                            <p>Programme Advisor: Dr. Li Peng</p>
                        </div>
                </div>

                <div class="section experience-item">
                    <!-- badge -->
                        <img src="images/badge/NUIST.jpg" alt="NUIST" class="institute-logo">
                
                    <!-- content -->
                        <div class="experience-content">
                            <h3><a href="https://en.nuist.edu.cn/" class="institute-title">Nanjing University of Information Science and Technology (NUIST)</a></h3>
                            <p class="time">2018.09 - 2022.06&nbsp;&nbsp;&bull;&nbsp;&nbsp;<span>Undergraduate student</span></p>
                            <p>GPA ranking: <span style="color:gray">2</span>/163</p>
                            <p>Research Advisor: Prof. <a href="https://sites.google.com/view/xiaolongxu">Xiaolong Xu</a></p>
                        </div>
                </div>
            </div>


            <div class="divider"></div>
            <div class="section awards and honors">
            <h1>Selected Awards and Honors</h1>
                <p>
                    <ul>
                        <li>2021: National Scholarship (<span style="color: gray;">Highest Honor</span> for Undergraduates in China, 8000RMB¬•)</li>
                        <li>2021: President Scholarship (<span style="color: gray;">Highest Honor</span> for Undergraduates in NUIST, 10000RMB¬•)</li>
                        <li>2021: Outstanding Students of Jiangsu Province, China</li>
                        <Li>2021: <span style="color: gray;">First Prize</span> in the National Finals of the China College Student Service Outsourcing Innovation and Entrepreneurship Competition</Li>
                        <li>2020: National Scholarship (<span style="color: gray;">Highest Honor</span> for Undergraduates in China, 8000RMB¬•)</li></li>
                        <li>2020: President Scholarship (<span style="color: gray;">Highest Honor</span> for Undergraduates in NUIST, 10000RMB¬•)</li>
                        <li>2020: <span style="color: gray;">Silver Award</span> in the Provincial Finals of ‚ÄúInternet +‚Äù College Students Innovation and Entrepreneurship Competition</li>
                        <li>2020: <span style="color: gray;">Second Prize</span> in Jiangsu Provincial College Student Robot Competition</li>
                    </ul>
                </p>
            </div>

<!-- 
            <div class="divider"></div>
            <div class="section miscellaneous">
            <h1>Miscellaneous</h1>
                <p>
                    <ul>
                        <li>Teaching Assistant: DATA130011 Neural Networks and Deep Learning, Spring 2024.</li>
                    </ul>
                </p>
            </div> -->

            <div class="divider"></div>
            <p style="text-align:center">
            <span style="color: gray;">¬© 2025 Tianyu Wang</span>
            </p>
        </div>
    </body>
</html>